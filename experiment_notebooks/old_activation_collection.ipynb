{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import os\n","from torch.utils.data import DataLoader, TensorDataset\n","import gc\n","from datasets import load_dataset\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","### Code to collect LLM activations compatible with Python 3.9's latest Transformers library\n","\n","\n","def preprocess(paragraphs, tokenizer, seq_len):\n","    token_ids = []\n","    for paragraph in paragraphs:\n","        ids = tokenizer(paragraph, add_special_tokens=False)[\"input_ids\"]\n","        token_ids.extend(ids)\n","\n","    tokenized = torch.tensor(token_ids, dtype=torch.long)\n","\n","    length = tokenized.size(0)\n","    if length >= seq_len:\n","        new_length = (length // seq_len) * seq_len\n","        tokenized = tokenized[:new_length]\n","        tokenized = tokenized.view(-1, seq_len)\n","\n","        bos = torch.full(\n","            (tokenized.size(0), 1), tokenizer.bos_token_id, dtype=torch.long\n","        )\n","        tokenized = torch.cat([bos, tokenized], dim=1)\n","\n","        target = tokenized.clone()\n","        target[:, :-1] = -100\n","        return tokenized, target\n","    else:\n","        raise ValueError(\"Dataset too small\")\n","\n","\n","def get_loader(dataset, tokenizer, batch_size, name=\"train\", seq_len=512, val=False):\n","    paragraphs = dataset[name][\"text\"]\n","    tokenized, target = preprocess(paragraphs, tokenizer, seq_len=seq_len)\n","    dataset = TensorDataset(tokenized, target)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n","    return dataloader\n","\n","\n","class HookManager:\n","    def __init__(self, model, model_name):\n","        self.hook_handles = []\n","        self.value_residuals = {}\n","        self.model = model\n","        self.model_name = model_name\n","\n","    def make_hook(self, name):\n","        def valres_hook(module, input, output):\n","            self.value_residuals[name] = output\n","\n","        return valres_hook\n","\n","    def register_hooks(self):\n","        if self.model_name == \"llama\":\n","            self.register_hooks_llama()\n","\n","    def register_hooks_llama(self):\n","        for i in range(0, len(self.model.model.layers)):\n","            if i % 5 == 0 or i == len(self.model.model.layers) - 1:\n","                handle_v = self.model.model.layers[\n","                    i\n","                ].self_attn.v_proj.register_forward_hook(self.make_hook(\"v\" + str(i)))\n","                handle_k = self.model.model.layers[\n","                    i\n","                ].self_attn.k_proj.register_forward_hook(self.make_hook(\"k\" + str(i)))\n","                handle_q = self.model.model.layers[\n","                    i\n","                ].self_attn.q_proj.register_forward_hook(self.make_hook(\"q\" + str(i)))\n","                self.hook_handles.append(handle_q)\n","                self.hook_handles.append(handle_v)\n","                self.hook_handles.append(handle_k)\n","        print(\"hooks registered\")\n","\n","    def clear_hooks(self):\n","        for handle in self.hook_handles:\n","            handle.remove()\n","        self.hook_handles.clear()\n","\n","    def clear_residuals(self):\n","        self.value_residuals.clear()\n","        del self.value_residuals\n","        torch.cuda.empty_cache()\n","        self.value_residuals = {}\n","        gc.collect()\n","\n","    def clear(self):\n","        self.clear_hooks()\n","        self.clear_residuals()\n","\n","\n","def archive_residuals(value_residuals, attentions, n_layers, id_batch):\n","    dump_dics = [{} for i in range(0, n_layers)]\n","    if attentions is not None:\n","        attentions = torch.stack(attentions).transpose(\n","            2, 1\n","        )  # output shape : (n_layers,n_heads,batch_size,seq_len,seq_len)\n","        for k in range(n_layers):\n","            dump_dics[k][\"attentions\"] = attentions[k]\n","    for i in range(0, n_layers):\n","        if i % 5 == 0 or i == n_layers - 1:\n","            v = value_residuals[\"v\" + str(i)].cpu().detach().contiguous()\n","            k = value_residuals[\"k\" + str(i)].cpu().detach().contiguous()\n","            q = value_residuals[\"q\" + str(i)].cpu().detach().contiguous()\n","            seqlen = v.shape[1]\n","            batchsize = v.shape[0]\n","            dump_dics[i][\"V\"] = (\n","                v.reshape(batchsize, seqlen, 32, 128).transpose(2, 1).transpose(1, 0)\n","            )\n","            dump_dics[i][\"K\"] = (\n","                k.reshape(batchsize, seqlen, 32, 128).transpose(2, 1).transpose(1, 0)\n","            )\n","            dump_dics[i][\"Q\"] = (\n","                q.reshape(batchsize, seqlen, 32, 128).transpose(2, 1).transpose(1, 0)\n","            )\n","            dump_dics[i][\"id_batch\"] = id_batch\n","    return dump_dics  # dim (n_layers,batch_size,seq_len,d)\n","\n","\n","def collect_activations(model, loader, output_attentions=False, stop_indice=5):\n","    model.eval()\n","    hook_manager = HookManager(model, \"llama\")\n","    n_layers = len(model.model.layers)\n","    for k in range(n_layers):\n","        d = \"layer\" + str(k)\n","        if not os.path.exists(\"layer\" + str(k)):\n","            os.makedirs(d)\n","    with torch.no_grad():\n","        for i, (data, target) in enumerate(loader):\n","            hook_manager.register_hooks()\n","            data, target = data.to(device), target.to(device)\n","            try:\n","                outputs = model(\n","                    input_ids=data, labels=target, output_attentions=output_attentions\n","                )\n","                try:\n","                    if output_attentions:\n","                        dump_dics = archive_residuals(\n","                            hook_manager.value_residuals,\n","                            outputs[\"attentions\"],\n","                            len(model.model.layers),\n","                            i,\n","                        )\n","                    else:\n","                        dump_dics = archive_residuals(\n","                            hook_manager.value_residuals, None, n_layers, i\n","                        )\n","                    for k in range(0, n_layers):\n","                        torch.save(\n","                            dump_dics[k], \"layer\" + str(k) + \"/dump\" + str(i) + \".pt\"\n","                        )\n","                    del dump_dics\n","                except Exception as e:\n","                    print(e)\n","                    print(\"Error with tensor saving\")\n","                del outputs\n","            except Exception as e:\n","                print(\"Error with model output\")\n","            hook_manager.clear()\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            if i == stop_indice:\n","                break\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if __name__ == \"__main__\":\n","    os.chdir(\"/data/mgiles/shil6478/activations\")\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model_id = f\"togethercomputer/LLaMA-2-7B-32K\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)\n","    model_8bit = AutoModelForCausalLM.from_pretrained(\n","        model_id, device_map=\"auto\", load_in_8bit=True\n","    )\n","    ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n","    train_loader = get_loader(ds, tokenizer, 2, seq_len=1024)\n","    collect_activations(\n","        model_8bit, train_loader, output_attentions=False, stop_indice=350\n","    )\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"}},"nbformat":4,"nbformat_minor":4}
